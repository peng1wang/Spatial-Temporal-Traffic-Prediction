{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Group the data into weekends and workdays first. Because from the time series data, the traffic flow on the weekend of July 6 and July 7 is relatively stable and less traffic flow, other weekend days are the same, need to be trained separately for modeling. The characteristics of traffic flow data are significantly different during weekends and weekdays, so ARIMA models are established separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nodes 4689-4662 are taken as the main nodes for analysis. The spatial adjacency matrix is constructed. The path directly connected with the road is the first-order adjacency matrix, and the path separated by two segments is the second-order adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "road_data = df[df['DEVICEID'] == 5696].copy()\n",
    "road_data['FROMTIME'] = pd.to_datetime(road_data['FROMTIME'])\n",
    "road_data.index = road_data['FROMTIME']\n",
    "road_data['FROMTIME-Time'] = road_data['FROMTIME'].apply(findTime)  # 找到时间\n",
    "road_data['FROMTIME-Date'] = road_data['FROMTIME'].apply(findDate)  # 找到日期\n",
    "xianquan = road_data.resample(datetime.timedelta(seconds=5 * 60)).sum()\n",
    "df = xianquan.copy()\n",
    "\n",
    "# Select the data from 7:00 to 22:00 as required by the problem\n",
    "df['FROMTIME'] = pd.to_datetime(df.index)\n",
    "df.index=df['FROMTIME']\n",
    "df['FROMTIME-Time'] = df[\"FROMTIME\"].apply(findTime)  # find time\n",
    "df['FROMTIME-Date'] = df[\"FROMTIME\"].apply(findDate)  # find date\n",
    "df=df[(pd.to_datetime(df[\"FROMTIME-Time\"],format = '%H:%M:%S')>= pd.to_datetime('07:00:00',format = '%H:%M:%S'))&(pd.to_datetime(df[\"FROMTIME-Time\"],format = '%H:%M:%S')<= pd.to_datetime('22:00:00',format = '%H:%M:%S'))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df1 is for weekdays and df2 is for weekends\n",
    "df1=df[(pd.to_datetime(df[\"FROMTIME-Date\"],format = '%Y-%m-%d')>= pd.to_datetime('2019-07-03',format = '%Y-%m-%d'))&(pd.to_datetime(df[\"FROMTIME-Date\"],format = '%Y-%m-%d')<= pd.to_datetime('2019-07-05',format = '%Y-%m-%d'))]\n",
    "df2=df[(pd.to_datetime(df[\"FROMTIME-Date\"],format = '%Y-%m-%d')>= pd.to_datetime('2019-07-06',format = '%Y-%m-%d'))&(pd.to_datetime(df[\"FROMTIME-Date\"],format = '%Y-%m-%d')<= pd.to_datetime('2019-07-07',format = '%Y-%m-%d'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# overall data for coil\n",
    "import matplotlib.dates as mdates\n",
    "fig=plt.figure(figsize= (20,5))\n",
    "ax = fig.add_subplot(1,1,1)  \n",
    "plt.plot(df.index,df['FLOW'])\n",
    "plt.title(str('5696'))\n",
    "ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize= (20,5))\n",
    "ax = fig.add_subplot(1,1,1)  \n",
    "plt.plot(df1.index,df1['FLOW'])\n",
    "plt.plot(df2.index,df2['FLOW'])\n",
    "plt.title(str('5696'))\n",
    "ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check whether the data is stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Looking at the data according to the plot, the visual sensation data has a certain seasonal\n",
    "df1[\"FLOW\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit root stationary test: The statistical properties of a given sequence (mean, variance, and covariance) are not constants of time, which is a prerequisite for a stationary time series. The variance of a stationary time series cannot be a function of time. The unit root test ** checks for the presence of a unit root in the sequence by checking the value of a=1 **. The following are the two most commonly used unit root stationary test methods: 1. Dickey Fuller test (ADF test); 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Perform ADF test \n",
    "Return value of adf_test \n",
    "Test statistic: indicates the test statistics \n",
    "p-value: indicates the probability of a P-value test \n",
    "Lags used: Lags are automatically selected when k, autolag=AIC is used \n",
    "Number of Observations Used: Number of samples \n",
    "Critical Value(5%) : A critical value with a significance level of 5%. \n",
    "(1) It is assumed that there is a unit root, that is, instability; \n",
    "(2) Significance level, 1% : strict rejection of null hypothesis; 5% : Reject null hypothesis, 10% by analogy. \n",
    "(3) Look at the size of P-value and significance level a, the smaller the P-value is, if it is less than the significance level, the null hypothesis is rejected and the series is considered to be stable; If it is greater than that, it cannot be rejected as unstable \n",
    "(4) Look at the test statistics and critical values, if the test statistics are less than the critical value, we reject the null hypothesis and think that the series is stable; If it is greater than that, it cannot be rejected as unstable\n",
    "'''\n",
    "## ADF test\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "def adf_test(timeseries):\n",
    "#     rolling_statistics(timeseries) # plot\n",
    "    print ('Results of Augment Dickey-Fuller Test:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value   # Increase the critical value of the significance level behind\n",
    "    print (dfoutput)\n",
    "    \n",
    "adf_test(df[\"FLOW\"])   # It can be seen from the results that the P-value is very small, rejecting the null hypothesis and thinking that it is a stationary sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# White noise test\n",
    "'''\n",
    "The acorr_ljungbox(x, lags=None, boxpierce=False) function checks for no autocorrelation \n",
    "lags represent the number of delay periods, if it is an integer, then the number of delay periods included, and if it is a list or array, then all lags are included in the largest lag in the list \n",
    "boxpierce True indicates that in addition to LB statistics, Box and Pierce's Q statistics are also returned \n",
    "Return value: \n",
    "lbvalue: statistics of the test\n",
    "bpvalue:((optionsal), float or array) – test statistic for Box-Pierce test\n",
    "bppvalue:((optional), float or array) – p-value based for Box-Pierce test on chi-square distribution\n",
    "'''\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "def test_stochastic(ts,lag):\n",
    "    p_value = acorr_ljungbox(ts, lags=lag) #lags can be self-defined\n",
    "    return p_value\n",
    "\n",
    "\n",
    "test_stochastic(df[\"FLOW\"],[6,12])# If p-value is less than 0.05, then the null hypothesis can be rejected as not a white noise sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine the order of ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Autocorrelation graph ACF and partial correlation graph PACF\n",
    "import statsmodels.api as sm\n",
    "def acf_pacf_plot(ts_log_diff):\n",
    "    sm.graphics.tsa.plot_acf(ts_log_diff,lags=40) #ARIMA,q\n",
    "    sm.graphics.tsa.plot_pacf(ts_log_diff,lags=40) #ARIMA,p\n",
    "    \n",
    "acf_pacf_plot(df[\"FLOW\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is a cyclical data, one cycle per day. That is, the interval is 15 hours (15*12=180), and there are 181 data a day from 7:00 to 22:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.index[543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1= fig.add_subplot(111)\n",
    "# Make first-order difference and re-difference for the same period data\n",
    "diff1= df1[\"FLOW\"].diff(181).diff(1)\n",
    "diff1=diff1.dropna()\n",
    "acf_pacf_plot(diff1)\n",
    "diff1.plot(ax=ax1)\n",
    "# After the first-order difference, the stationarity test has been carried out. It can be seen from the ACF and PACF diagrams that ACF8 order is truncated and PACF8 order is truncated. ARIMA (8,1,8)\n",
    "adf_test(diff1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "model = ARIMA(diff1,order=(8,0,6)) \n",
    "results_AR = model.fit(disp=-1)\n",
    "prediction=results_AR.fittedvalues\n",
    "plt.plot(diff1)\n",
    "plt.plot(prediction, color='red')\n",
    "sum((prediction[2:]-diff1[2:])**2)/diff1.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forecast= results_AR.predict(start =363, end= 724, dynamic= True)  \n",
    "df3=df[(pd.to_datetime(df[\"FROMTIME-Date\"],format = '%Y-%m-%d')>= pd.to_datetime('2019-07-13',format = '%Y-%m-%d'))&(pd.to_datetime(df[\"FROMTIME-Date\"],format = '%Y-%m-%d')<= pd.to_datetime('2019-07-14',format = '%Y-%m-%d'))]\n",
    "df3.plot()\n",
    "forecast.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_ts = results_AR.predict()\n",
    "\n",
    "diff_shift_ts = diff1.shift(1)\n",
    "diff_recover_1 = predict_ts.add(diff_shift_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_recover_1.plot(color='blue', label='Predict')\n",
    "df[\"FLOW\"].plot(color='red', label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.title('RMSE: %.4f'% np.sqrt(sum((diff_recover_1-df[\"FLOW\"])**2)/df[\"FLOW\"].size))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff1= df[\"FLOW\"].diff(1).diff(1)\n",
    "diff1=diff1.dropna()\n",
    "model = ARIMA(df[\"FLOW\"], order=(0, 2, 1))  \n",
    "results_AR = model.fit(disp=-1) \n",
    "plt.plot(diff1)\n",
    "prediction=results_AR.fittedvalues\n",
    "plt.plot(prediction, color='green')\n",
    "plt.title('RSS: %.4f'% sum((prediction[2:]-diff1[2:])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_restored = pd.Series([time_series[0]], index=[time_series.index[0]]) .append(time_series_diff).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff1= df[\"FLOW\"].diff(2)\n",
    "model = ARIMA(df[\"FLOW\"], order=(0, 2, 1))  \n",
    "results_ARIMA = model.fit(disp=-1)  \n",
    "plt.plot(diff1)\n",
    "plt.plot(results_MA.fittedvalues, color='green')\n",
    "plt.title('RSS: %.4f'% sum((results_MA.fittedvalues-diff1[2:])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_shift_ts = diff1.shift(1)\n",
    "diff_recover_1 = results_MA.fittedvalues.add(diff_shift_ts)\n",
    "\n",
    "rol_sum = ts_log.rolling(window=1).sum()\n",
    "rol_recover = diff_recover*12 - rol_sum.shift(1)\n",
    "\n",
    "rol_recover.plot(color='blue', label='Predict')\n",
    "df[\"FLOW\"].plot(color='red', label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.title('RMSE: %.4f'% np.sqrt(sum((rol_recover-df[\"FLOW\"])**2)/ts.size))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=df\n",
    "plt.plot(df[\"FLOW\"])\n",
    "plt.plot(results_AR.fittedvalues, color='red') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decompose\n",
    "\n",
    "def decompose(timeseries):\n",
    "\n",
    "    # return contains three part: trend， seasonal and residual\n",
    "    decomposition = seasonal_decompose(timeseries,freq=156)\n",
    "    trend = decomposition.trend\n",
    "    seasonal = decomposition.seasonal\n",
    "    residual = decomposition.resid\n",
    "    \n",
    "    plt.subplot(411)\n",
    "    plt.plot(df[\"FLOW\"], label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(412)\n",
    "    plt.plot(trend, label='Trend')\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(413)\n",
    "    plt.plot(seasonal,label='Seasonality')\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplot(414)\n",
    "    plt.plot(residual, label='Residuals')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return trend , seasonal, residual\n",
    "trend , seasonal, residual = decompose(df[\"FLOW\"])\n",
    "residual.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTime(x):\n",
    "    return x.time()\n",
    "def findDate(x):\n",
    "    return x.date()\n",
    "\n",
    "\n",
    "filelist = os.listdir('measurement')\n",
    "filelist.sort()\n",
    "startpoint = np.where(np.array(filelist) == '16157')[0][0]\n",
    "filelist = filelist[startpoint:]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for fileid in filelist:\n",
    "    data = pd.read_csv('measurement/'+fileid+'/tab_ldt.csv')\n",
    "    df = pd.concat([df,data])\n",
    "\n",
    "\n",
    "for roadid in df['DEVICEID'].unique(): \n",
    "    road_data = df[df['DEVICEID'] == roadid].copy()\n",
    "    road_data['FROMTIME'] = pd.to_datetime(road_data['FROMTIME'])\n",
    "    road_data.index = road_data['FROMTIME']\n",
    "    road_data['FROMTIME-Time'] = road_data['FROMTIME'].apply(findTime) \n",
    "    road_data['FROMTIME-Date'] = road_data['FROMTIME'].apply(findDate) \n",
    "    road_data.to_csv('train_data/data/'+'%s.csv'%roadid)\n",
    "    plt.figure(figsize= (20,5))\n",
    "    plt.title(str(roadid))\n",
    "    for date in road_data['FROMTIME-Date'].unique():\n",
    "        df_temp = road_data[road_data['FROMTIME-Date'] == date]\n",
    "        data3 = df_temp.resample(datetime.timedelta(seconds=5 * 60)).sum()\n",
    "        data3.index = pd.Series(data3.index).apply(findTime)\n",
    "        plt.plot(data3.index,data3['FLOW'],label=date)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

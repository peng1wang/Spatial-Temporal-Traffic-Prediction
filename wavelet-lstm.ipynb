{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import itertools\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTime(x):\n",
    "    return x.time()\n",
    "def findDate(x):\n",
    "    return x.date()\n",
    "\n",
    "road_data = pd.read_csv('train_data/data/5684.csv')\n",
    "xianquan_str='5684'\n",
    "road_data['FROMTIME'] = pd.to_datetime(road_data['FROMTIME'])\n",
    "road_data.index = road_data['FROMTIME']\n",
    "road_data['FROMTIME-Time'] = road_data['FROMTIME'].apply(findTime)  # 找到时间\n",
    "road_data['FROMTIME-Date'] = road_data['FROMTIME'].apply(findDate)  # 找到日期\n",
    "road_data\n",
    "\n",
    "plt.figure(figsize= (20,5))\n",
    "plt.title(str(5684))\n",
    "for date in road_data['FROMTIME-Date'].unique():\n",
    "    df_temp = road_data[road_data['FROMTIME-Date'] == date]\n",
    "    data3 = df_temp.resample(datetime.timedelta(seconds=5 * 60)).sum()\n",
    "    data3.index = pd.Series(data3.index).apply(findTime)\n",
    "    plt.plot(data3.index,data3['FLOW'],label=date)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the data into weekend and weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM,Bidirectional\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import regularizers\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xianquan = road_data.resample(datetime.timedelta(seconds=5 * 60)).sum()\n",
    "df = xianquan.copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['FROMTIME'] = pd.to_datetime(df.index)\n",
    "df['FROMTIME-Time'] = df[\"FROMTIME\"].apply(findTime)  # 找到时间\n",
    "df['FROMTIME-Date'] = df[\"FROMTIME\"].apply(findDate)  # 找到日期\n",
    "df=df[(pd.to_datetime(df[\"FROMTIME-Time\"],format = '%H:%M:%S')>= pd.to_datetime('07:00:00',format = '%H:%M:%S'))&(pd.to_datetime(df[\"FROMTIME-Time\"],format = '%H:%M:%S')<= pd.to_datetime('22:00:00',format = '%H:%M:%S'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "weekend_list=[\"2019-07-06\",\"2019-07-07\",\"2019-07-13\",\"2019-07-14\",\"2019-07-20\",\"2019-07-21\",\"2019-07-27\",\"2019-07-28\"]\n",
    "\n",
    "df_weekend=pd.DataFrame()\n",
    "for day in weekend_list:\n",
    "    temp=df[pd.to_datetime(df['FROMTIME-Date'],format = '%Y-%m-%d')==pd.to_datetime(day,format = '%Y-%m-%d')]\n",
    "    df_weekend=pd.concat([df_weekend,temp])\n",
    "\n",
    "df_weekend.index = df_weekend['FROMTIME']\n",
    "df_weekend.to_csv('train_data/lstm/weekend_%s.csv'%xianquan_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weekday_list=[\"2019-07-01\",\"2019-07-02\",\"2019-07-03\",\"2019-07-04\",\"2019-07-05\",\"2019-07-08\",\"2019-07-09\",\"2019-07-10\",\n",
    "             \"2019-07-11\",\"2019-07-12\",\"2019-07-15\",\"2019-07-16\",\"2019-07-17\",\"2019-07-18\",\"2019-07-19\",\"2019-07-22\",\n",
    "             \"2019-07-23\",\"2019-07-24\",\"2019-07-25\",\"2019-07-26\",\"2019-07-29\",\"2019-07-30\",\"2019-07-31\"]\n",
    "df_weekday=pd.DataFrame()\n",
    "for day in weekday_list:\n",
    "    temp=df[pd.to_datetime(df['FROMTIME-Date'],format = '%Y-%m-%d')==pd.to_datetime(day,format = '%Y-%m-%d')]\n",
    "    df_weekday=pd.concat([df_weekday,temp])\n",
    "\n",
    "df_weekday.index = df_weekday['FROMTIME']\n",
    "df_weekday.to_csv('train_data/lstm/weekday_%s.csv'%xianquan_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (20,5))\n",
    "plt.title(str('%s'%xianquan_str))\n",
    "# plt.plot(df_weekend.index,df_weekend['COUNT'])\n",
    "# plt.plot(df_weekday.index,df_weekday['COUNT'])\n",
    "plt.plot(df['COUNT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet denoising decomposes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pywt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wavelet noise filtering\n",
    "def wavelet_denoising(data):\n",
    "    #  wavelet function takes db4\n",
    "    db4 = pywt.Wavelet('db4')\n",
    "    #  decompose\n",
    "    coeffs = pywt.wavedec(data, db4)\n",
    "    # high frequency coefficient is zeroed\n",
    "    coeffs[len(coeffs)-1] *= 0\n",
    "    coeffs[len(coeffs)-2] *= 0\n",
    "    # resconstruct\n",
    "    meta = pywt.waverec(coeffs, db4)\n",
    "    return meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train for weekend data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_weekend=pd.read_csv('train_data/lstm/weekend_%s.csv'%xianquan_str)\n",
    "df_weekday=pd.read_csv('train_data/lstm/weekday_%s.csv'%xianquan_str)\n",
    "\n",
    "df_weekday.index=df_weekday[\"FROMTIME\"]\n",
    "df_weekend.index=df_weekend[\"FROMTIME\"]\n",
    "\n",
    "data1=df_weekend[(pd.to_datetime(df_weekend.index,format = '%Y-%m-%d %H:%M:%S')<= pd.to_datetime('2019-07-27 22:00:00',format = '%Y-%m-%d %H:%M:%S'))]\n",
    "data2=df_weekend[(pd.to_datetime(df_weekend.index,format = '%Y-%m-%d %H:%M:%S')> pd.to_datetime('2019-07-27 22:00:00',format = '%Y-%m-%d %H:%M:%S'))]\n",
    "data1.to_csv('train_data/train_%s.csv'%xianquan_str)\n",
    "data2.to_csv('train_data/test_%s.csv'%xianquan_str)\n",
    "\n",
    "data3=df_weekday[(pd.to_datetime(df_weekday.index,format = '%Y-%m-%d %H:%M:%S')<= pd.to_datetime('2019-07-30 22:00:00',format = '%Y-%m-%d %H:%M:%S'))]\n",
    "data4=df_weekday[(pd.to_datetime(df_weekday.index,format = '%Y-%m-%d %H:%M:%S')> pd.to_datetime('2019-07-30 22:00:00',format = '%Y-%m-%d %H:%M:%S'))]\n",
    "data3.to_csv('train_data/train_%s.csv'%xianquan_str)\n",
    "data4.to_csv('train_data/test_%s.csv'%xianquan_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lag = 12\n",
    "file1 = 'train_data/train_%s.csv'%xianquan_str\n",
    "file2 = 'train_data/test_%s.csv'%xianquan_str\n",
    "\n",
    "df1 = pd.read_csv(file1, encoding='utf-8').fillna(0)\n",
    "df2 = pd.read_csv(file2, encoding='utf-8').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training data: data1_temp is the low frequency band of weekend data, and data1_zao is the noise segment of weekend data\n",
    "data_list=np.array(df1[\"COUNT\"])\n",
    "\n",
    "data=[]\n",
    "for i in range(len(data_list)):\n",
    "    if data_list[i]>0.01:\n",
    "        data.append(data_list[i])\n",
    "data_list=data\n",
    "\n",
    "data1_temp=wavelet_denoising(data_list)\n",
    "data1_zao=[]\n",
    "for i in range(len(data_list)):\n",
    "    tmp=data_list[i]-data1_temp[i]\n",
    "    data1_zao.append(tmp)\n",
    "data1_zao=np.array(data1_zao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test data: data2_temp is the low frequency band of weekend data, and data2_zao is the noise segment of weekend data\n",
    "data_list2=np.array(df2[\"COUNT\"])\n",
    "data2_temp=wavelet_denoising(data_list2)\n",
    "df_data2 = np.array(df2[\"COUNT\"])\n",
    "data2_zao=[]\n",
    "for i in range(len(df_data2)):\n",
    "    tmp=df_data2[i]-data2_temp[i]\n",
    "    data2_zao.append(tmp)\n",
    "data2_zao=np.array(data2_zao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Noise data to be trained\n",
    "plt.figure(figsize= (20,5))\n",
    "plt.title(str('%s'%xianquan_str))\n",
    "plt.plot(data1_zao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Low frequency data to be trained\n",
    "plt.figure(figsize= (20,5))\n",
    "plt.title(str('%s'%xianquan_str))\n",
    "plt.plot(data1_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prapare training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(train, test, lags):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(train.reshape(-1, 1))\n",
    "    flow1 = scaler.transform(train.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    flow2 = scaler.transform(test.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "    train, test = [], []\n",
    "    for i in range(lags, len(flow1)):\n",
    "        if (i/12)%15==0:\n",
    "            if i==180:\n",
    "                temp=flow1[i - lags: i + 12]\n",
    "                if len(temp)==24:\n",
    "                    train.append(temp)\n",
    "        else:\n",
    "            \n",
    "            temp=flow1[i - lags: i + 12]\n",
    "            if len(temp)==24:\n",
    "                train.append(temp)\n",
    "                \n",
    "    for i in range(lags, len(flow2),12):\n",
    "        if (i/12)%16==0:\n",
    "            pass\n",
    "        else:\n",
    "            temp=flow2[i - lags: i + 12]\n",
    "            if len(temp)==24:\n",
    "                test.append(temp)\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    np.random.shuffle(train)\n",
    "    X_train = train[:,0:12]\n",
    "    y_train = train[:,-12:]\n",
    "    X_test = test[:,0:12]\n",
    "    y_test = test[:,-12:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler\n",
    "\n",
    "\n",
    "\n",
    "def process_data_zao(train, test, lags):\n",
    "    scaler_zao = MinMaxScaler(feature_range=(0, 1)).fit(train.reshape(-1, 1))\n",
    "    flow1 = scaler_zao.transform(train.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "    flow2 = scaler_zao.transform(test.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "    train, test = [], []\n",
    "    for i in range(lags, len(flow1)):\n",
    "        if (i/12)%15==0:\n",
    "            if i==180:\n",
    "                temp=flow1[i - lags: i + 12]\n",
    "                if len(temp)==24:\n",
    "                    train.append(temp)\n",
    "        else:\n",
    "           \n",
    "            temp=flow1[i - lags: i + 12]\n",
    "            if len(temp)==24:\n",
    "                train.append(temp)\n",
    "                \n",
    "    for i in range(lags, len(flow2),12):\n",
    "        if (i/12)%16==0:\n",
    "            pass\n",
    "        else:\n",
    "            temp=flow2[i - lags: i + 12]\n",
    "            if len(temp)==24:\n",
    "                test.append(temp)\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    np.random.shuffle(train)\n",
    "    X_train = train[:,0:12]\n",
    "    y_train = train[:,-12:]\n",
    "    X_test = test[:,0:12]\n",
    "    y_test = test[:,-12:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler_zao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lstm(units):\n",
    "    \"\"\"LSTM(Long Short-Term Memory)\n",
    "    Build LSTM Model.\n",
    "\n",
    "    # Arguments\n",
    "        units: List(int), number of input, output and hidden units.\n",
    "    # Returns\n",
    "        model: Model, nn model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units[1], activation='tanh',input_shape=(units[0], 1),return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(LSTM(units[2],activation='tanh'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(units=units[3],activation='linear')) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    " Training: LSTM and GRU are trained according to the normal RNN network. Use the train_model() function to train. \n",
    " \n",
    "RMSprop(lr=0.001, rho=0.9, epsilon=1e-06) was used as the optimizer with batch_szie of 256 and lags of 12(i.e., lags of one hour).\n",
    "\"\"\"\n",
    "def train_model(model, X_train, y_train, name, config,wd_str,is_zao):\n",
    "    \"\"\"train\n",
    "    train a single model.\n",
    "\n",
    "    # Arguments\n",
    "        model: Model, NN model to train.\n",
    "        X_train: ndarray(number, lags), Input data for train.\n",
    "        y_train: ndarray(number, ), result data for train.\n",
    "        name: String, name of model.\n",
    "        config: Dict, parameter for train.\n",
    "    \"\"\"\n",
    "\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=['mape'])#optimizer原先是rmsprop\n",
    "    model.summary()\n",
    "    # early = EarlyStopping(monitor='val_loss', patience=30, verbose=0, mode='auto')\n",
    "    hist = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=config[\"batch\"],\n",
    "        epochs=config[\"epochs\"],\n",
    "        validation_split=0.05)# original parameter was 0.05\n",
    "    if is_zao==False:\n",
    "        if wd_str=='weekday':\n",
    "            model.save('train_data/lstm/weekday_model/xiaobo%s.h5'%xianquan_str)\n",
    "            df = pd.DataFrame.from_dict(hist.history)\n",
    "            df.to_csv('train_data/lstm/weekday_model/xiaobo%sloss.csv'%xianquan_str, encoding='utf-8', index=False)\n",
    "        else:\n",
    "            model.save('train_data/lstm/weekend_model/xiaobo%s.h5'%xianquan_str)\n",
    "            df = pd.DataFrame.from_dict(hist.history)\n",
    "            df.to_csv('train_data/lstm/weekend_model/xiaobo%sloss.csv'%xianquan_str, encoding='utf-8', index=False)  \n",
    "    else:\n",
    "        if wd_str=='weekday':\n",
    "            model.save('train_data/lstm/weekday_model/xiaobo_zao%s.h5'%xianquan_str)\n",
    "            df = pd.DataFrame.from_dict(hist.history)\n",
    "            df.to_csv('train_data/lstm/weekday_model/xiaobo_zao%sloss.csv'%xianquan_str, encoding='utf-8', index=False)\n",
    "        else:\n",
    "            model.save('train_data/lstm/weekend_model/xiaobo_zao%s.h5'%xianquan_str)\n",
    "            df = pd.DataFrame.from_dict(hist.history)\n",
    "            df.to_csv('train_data/lstm/weekend_model/xiaobo_zao%sloss.csv'%xianquan_str, encoding='utf-8', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Percentage Error\n",
    "    Calculate the mape.\n",
    "\n",
    "    # Arguments\n",
    "        y_true: List/ndarray, ture data.\n",
    "        y_pred: List/ndarray, predicted data.\n",
    "    # Returns\n",
    "        mape: Double, result data for train.\n",
    "    \"\"\"\n",
    "\n",
    "    y = [x for x in y_true if x > 0]\n",
    "    y_pred = [y_pred[i] for i in range(len(y_true)) if y_true[i] > 0]\n",
    "\n",
    "    num = len(y_pred)\n",
    "    sums = 0\n",
    "\n",
    "    for i in range(num):\n",
    "        tmp = abs(y[i] - y_pred[i]) / y[i]\n",
    "        sums += tmp\n",
    "    if num!=0:\n",
    "        mape = sums * (100 / num)\n",
    "    else:\n",
    "        mape=0\n",
    "    return mape\n",
    "\n",
    "\n",
    "def eva_regress(y_true, y_pred):\n",
    "    \"\"\"Evaluation\n",
    "    evaluate the predicted resul.\n",
    "\n",
    "    # Arguments\n",
    "        y_true: List/ndarray, ture data.\n",
    "        y_pred: List/ndarray, predicted data.\n",
    "    \"\"\"\n",
    "\n",
    "    mape = MAPE(y_true, y_pred)\n",
    "    vs = metrics.explained_variance_score(y_true, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    print('explained_variance_score:%f' % vs)\n",
    "    print('mape:%f%%' % mape)\n",
    "    print('mae:%f' % mae)\n",
    "    print('mse:%f' % mse)\n",
    "    print('rmse:%f' % math.sqrt(mse))\n",
    "    print('r2:%f' % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, scaler= process_data(data1_temp, data2_temp, lag)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Low frequency data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_str='weekend'\n",
    "is_zao=False\n",
    "\n",
    "units=[X_train.shape[1],64,128,y_train.shape[1]]\n",
    "model=get_lstm(units)\n",
    "name='lstm'\n",
    "config={\"batch\":64,\"epochs\":500}\n",
    "train_model(model, X_train, y_train, name, config,wd_str,is_zao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xiaobolstm = load_model('train_data/lstm/weekend_model/xiaobo%s.h5'%xianquan_str)\n",
    "\n",
    "model=xiaobolstm\n",
    "name='xiaoboLSTM'\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler= process_data(data1_temp, data2_temp, lag)\n",
    "\n",
    "y_test=y_test.reshape(y_test.shape[0],y_test.shape[1])\n",
    "y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "y_preds = []\n",
    "train_preds=[]\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "file = 'train_data/model' + name + '.png'\n",
    "plot_model(model, to_file=file, show_shapes=True)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "predicted = scaler.inverse_transform(predicted)\n",
    "\n",
    "# y_preds.append(predicted)#[:12]\n",
    "y_preds=predicted\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = '2019-07-08 08:00'\n",
    "x = pd.date_range(d, periods=168, freq='5min')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "y_test_temp=[]\n",
    "for i in range(len(y_test)):\n",
    "    for j in range(len(y_test[i])):\n",
    "        y_test_temp.append(y_test[i][j])\n",
    "\n",
    "# y_test_true=[]\n",
    "# for i in range(len(y_test)):\n",
    "#     for j in range(len(y_test[i])):\n",
    "#         tmp=y_test[i][j]+y_test_zao[i][j]\n",
    "#         y_test_true.append(tmp)\n",
    "    \n",
    "ax.plot(x, y_test_temp, label='True Data')\n",
    "# ax.plot(x, y_test_true, label='True Data')\n",
    "\n",
    "y_preds_temp=[]\n",
    "for i in range(len(y_preds)):\n",
    "    for j in range(len(y_preds[i])):\n",
    "        y_preds_temp.append(y_preds[i][j])\n",
    "        \n",
    "ax.plot(x, y_preds_temp, label='preds Data')\n",
    "\n",
    "\n",
    "eva_regress(y_test_temp, y_preds_temp)\n",
    "# eva_regress(y_test_true, y_preds_temp)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Flow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_zao, y_train_zao, X_test_zao, y_test_zao, scaler_zao= process_data_zao(data1_zao, data2_zao, lag)\n",
    "X_train_zao = np.reshape(X_train_zao, (X_train_zao.shape[0], X_train_zao.shape[1], 1))\n",
    "\n",
    "d = '2019-07-08 08:00'\n",
    "x = pd.date_range(d, periods=168, freq='5min')\n",
    "\n",
    "y_test_zao=y_test_zao.reshape(y_test_zao.shape[0],y_test_zao.shape[1])\n",
    "y_test_zao = scaler_zao.inverse_transform(y_test_zao)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "y_test_temp=[]\n",
    "for i in range(len(y_test)):\n",
    "    for j in range(len(y_test[i])):\n",
    "        y_test_temp.append(y_test[i][j])\n",
    "\n",
    "\n",
    "y_test_true=[]\n",
    "for i in range(len(y_test)):\n",
    "    for j in range(len(y_test[i])):\n",
    "        tmp=y_test[i][j]+y_test_zao[i][j]\n",
    "        y_test_true.append(tmp)\n",
    "    \n",
    "# ax.plot(x, y_test_temp, label='True Data')\n",
    "ax.plot(x, y_test_true, label='True Data')\n",
    "\n",
    "y_preds_temp=[]\n",
    "for i in range(len(y_preds)):\n",
    "    for j in range(len(y_preds[i])):\n",
    "        y_preds_temp.append(y_preds[i][j])\n",
    "        \n",
    "y_preds_true=[]\n",
    "for i in range(len(y_preds)):\n",
    "    for j in range(len(y_preds[i])):\n",
    "        tmp=y_preds[i][j]+y_preds_zao[i][j]\n",
    "        y_preds_true.append(tmp)\n",
    "\n",
    "ax.plot(x, y_preds_true, label='preds Data')\n",
    "\n",
    "\n",
    "# eva_regress(y_test_temp, y_preds_temp)\n",
    "eva_regress(y_test_true, y_preds_true)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Flow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add attention mechanism to train low frequency data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, LSTM, merge ,Conv1D,Dropout,Bidirectional,Multiply\n",
    "from keras.models import Model\n",
    "from keras.layers import merge\n",
    "from keras.layers.core import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import *\n",
    "import  pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print('shape为',layer_activations.shape)\n",
    "            print(layer_activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_3d_block2(inputs, single_attention_vector=False):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    time_steps = K.int_shape(inputs)[1]\n",
    "    input_dim = K.int_shape(inputs)[2]\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    if single_attention_vector:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "\n",
    "    # element-wise\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul\n",
    "\n",
    "def attention_model():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIMS))\n",
    "\n",
    "    x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    #lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "    lstm_out = Dropout(0.3)(lstm_out)\n",
    "    attention_mul = attention_3d_block2(lstm_out,True)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "    output = Dense(12, activation='sigmoid')(attention_mul)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model\n",
    "\n",
    "INPUT_DIMS = 1\n",
    "TIME_STEPS = 12\n",
    "lstm_units = 64\n",
    "wd_str='weekend'\n",
    "m = attention_model()\n",
    "m.summary()\n",
    "m.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler= process_data(data1_temp, data2_temp, lag)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "m.fit([X_train], y_train, epochs=1000, batch_size=32, validation_split=0.1)\n",
    "\n",
    "if wd_str=='weekday':\n",
    "    m.save('train_data/lstm/weekday_model/xiaobo_attention_%s.h5'%xianquan_str)\n",
    "else:\n",
    "    m.save('train_data/lstm/weekend_model/xiaobo_attention_%s.h5'%xianquan_str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xiaobolstm = load_model('train_data/lstm/weekend_model/xiaobo_attention_%s.h5'%xianquan_str)\n",
    "\n",
    "model=xiaobolstm\n",
    "name='xiaobo_attentionLSTM'\n",
    "\n",
    "X_train, y_train, X_test, y_test, scaler= process_data(data1_temp, data2_temp, lag)\n",
    "\n",
    "y_test=y_test.reshape(y_test.shape[0],y_test.shape[1])\n",
    "y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "y_preds = []\n",
    "train_preds=[]\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "file = 'train_data/model' + name + '.png'\n",
    "plot_model(model, to_file=file, show_shapes=True)\n",
    "predicted = model.predict(X_test)\n",
    "train_predicted=model.predict(X_train)\n",
    "predicted = scaler.inverse_transform(predicted)\n",
    "train_predicted = scaler.inverse_transform(train_predicted)\n",
    "# y_preds.append(predicted)#[:12]\n",
    "y_preds=predicted\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = '2019-07-08 08:00'\n",
    "x = pd.date_range(d, periods=168, freq='5min')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "y_test_temp=[]\n",
    "for i in range(len(y_test)):\n",
    "    for j in range(len(y_test[i])):\n",
    "        y_test_temp.append(y_test[i][j])\n",
    "\n",
    "# y_test_true=[]\n",
    "# for i in range(len(y_test)):\n",
    "#     for j in range(len(y_test[i])):\n",
    "#         tmp=y_test[i][j]+y_test_zao[i][j]\n",
    "#         y_test_true.append(tmp)\n",
    "    \n",
    "ax.plot(x, y_test_temp, label='True Data')\n",
    "# ax.plot(x, y_test_true, label='True Data')\n",
    "\n",
    "y_preds_temp=[]\n",
    "for i in range(len(y_preds)):\n",
    "    for j in range(len(y_preds[i])):\n",
    "        y_preds_temp.append(y_preds[i][j])\n",
    "        \n",
    "ax.plot(x, y_preds_temp, label='preds Data')\n",
    "\n",
    "\n",
    "eva_regress(y_test_temp, y_preds_temp)\n",
    "# eva_regress(y_test_true, y_preds_temp)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Flow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise data training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_zao, y_train_zao, X_test_zao, y_test_zao, scaler_zao= process_data_zao(data1_zao, data2_zao, lag)\n",
    "X_train_zao = np.reshape(X_train_zao, (X_train_zao.shape[0], X_train_zao.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd_str='weekend'\n",
    "is_zao=True\n",
    "\n",
    "units=[X_train_zao.shape[1],64,64,y_train_zao.shape[1]]\n",
    "model=get_lstm(units)\n",
    "name='xiaobo_lstm'\n",
    "config={\"batch\":64,\"epochs\":2000}\n",
    "train_model(model, X_train_zao, y_train_zao, name, config,wd_str,is_zao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xiaobolstm_zao = load_model('train_data/lstm/weekend_model/xiaobo_zao%s.h5'%xianquan_str)\n",
    "\n",
    "model=xiaobolstm_zao\n",
    "name='xiaoboLSTM_zao'\n",
    "\n",
    "X_train_zao, y_train_zao, X_test_zao, y_test_zao, scaler_zao=process_data_zao(data1_zao, data2_zao, lag)\n",
    "\n",
    "y_test_zao=y_test_zao.reshape(y_test_zao.shape[0],y_test_zao.shape[1])\n",
    "y_test_zao = scaler_zao.inverse_transform(y_test_zao)\n",
    "\n",
    "y_preds = []\n",
    "train_preds=[]\n",
    "\n",
    "X_test_zao = np.reshape(X_test_zao, (X_test_zao.shape[0], X_test_zao.shape[1], 1))\n",
    "\n",
    "file = 'train_data/model' + name + '.png'\n",
    "plot_model(model, to_file=file, show_shapes=True)\n",
    "predicted = model.predict(X_test_zao)\n",
    "predicted = scaler_zao.inverse_transform(predicted)\n",
    "# y_preds.append(predicted)#[:12]\n",
    "y_preds_zao=predicted\n",
    "\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(y_test_zao)):\n",
    "    plt.figure()\n",
    "    plt.plot(y_test_zao[i])\n",
    "    plt.plot(y_preds_zao[i])\n",
    "    eva_regress(y_test_zao[i], y_preds_zao[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "import importlib\n",
    "importlib.reload(sys)\n",
    "\n",
    "\n",
    "def process_data2(train, test, lags):\n",
    "    attr = 'COUNT'\n",
    "    df1 = pd.read_csv(train, encoding='utf-8').fillna(0)\n",
    "    df2 = pd.read_csv(test, encoding='utf-8').fillna(0)\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1)).fit(df1[attr].values.reshape(-1, 1))\n",
    "    flow1 = scaler.transform(df1[attr].values.reshape(-1, 1))\n",
    "    flow2 = scaler.transform(df2[attr].values.reshape(-1, 1))\n",
    "\n",
    "    train, test = [], []\n",
    "\n",
    "    for i in range(lags, len(flow1)):\n",
    "        if (i/12)%15==0:\n",
    "            if i==180:\n",
    "                temp=flow1[i - lags: i + 12]\n",
    "                if len(temp)==24:\n",
    "                    train.append(temp)\n",
    "        else:\n",
    "            temp=flow1[i - lags: i + 12]\n",
    "            if len(temp)==24:\n",
    "                train.append(temp)\n",
    "                \n",
    "    for i in range(lags, len(flow2),12):\n",
    "        if (i/12)%16==0:\n",
    "            pass\n",
    "        else:\n",
    "            temp=flow2[i - lags: i + 12]\n",
    "            if len(temp)==24:\n",
    "                test.append(temp)\n",
    "\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    np.random.shuffle(train)\n",
    "    X_train = train[:,0:12]\n",
    "    y_train = train[:,-12:]\n",
    "    X_test = test[:,0:12]\n",
    "    y_test = test[:,-12:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm = load_model('train_data/lstm/weekend_model/xiaobo%s.h5'%xianquan_str)\n",
    "\n",
    "model=lstm\n",
    "name='LSTM'\n",
    "\n",
    "lag = 12\n",
    "file1 = 'train_data/train_%s.csv'%xianquan_str\n",
    "file2 = 'train_data/test_%s.csv'%xianquan_str\n",
    "_, _, X_test, y_test, scaler = process_data(file1, file2, lag)\n",
    "\n",
    "y_test=y_test.reshape(y_test.shape[0],y_test.shape[1])\n",
    "y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "y_preds = []\n",
    "train_preds=[]\n",
    "\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "file = 'train_data/model' + name + '.png'\n",
    "plot_model(model, to_file=file, show_shapes=True)\n",
    "predicted = model.predict(X_test)\n",
    "train_predicted=model.predict(X_train)\n",
    "predicted = scaler.inverse_transform(predicted)\n",
    "train_predicted = scaler.inverse_transform(train_predicted)\n",
    "# y_preds.append(predicted)#[:12]\n",
    "y_preds=predicted\n",
    "\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(y_test)):\n",
    "    plt.figure()\n",
    "    plt.plot(y_test[i])\n",
    "    plt.plot(y_preds[i])\n",
    "    eva_regress(y_test[i], y_preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "wd_str='weekend'\n",
    "\n",
    "units=[X_train.shape[1],64,128,y_train.shape[1]]\n",
    "model=get_lstm(units)\n",
    "name='lstm'\n",
    "config={\"batch\":200,\"epochs\":2000}\n",
    "train_model(model, X_train, y_train, name, config,wd_str)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

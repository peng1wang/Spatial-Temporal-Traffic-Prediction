{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import itertools\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Space-time prediction idea: Construct the first order adjacency matrix of the road segment to be predicted, and obtain the decomposed time part by singular value decomposition; The network structure of cnn+lstm+attention was used for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data of each coil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the coils of nodes 4697-4806(5690) for prediction. The coils required for the construction of the first order adjacency matrix are: 1.4706-4697(5684) coils; 2.4651-4697(6135); 3. 4694-4697(6132) coil. The four coils form a spatial adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_5690=pd.read_csv('train_data/data/5690.csv')\n",
    "data_5684=pd.read_csv('train_data/data/5684.csv')\n",
    "data_6135=pd.read_csv('train_data/data/6135.csv')\n",
    "data_6132=pd.read_csv('train_data/data/6132.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTime(x):\n",
    "    return x.time()\n",
    "def findDate(x):\n",
    "    return x.date()\n",
    "\n",
    "data_5690['FROMTIME'] = pd.to_datetime(data_5690['FROMTIME'])\n",
    "data_5684['FROMTIME'] = pd.to_datetime(data_5684['FROMTIME'])\n",
    "data_6135['FROMTIME'] = pd.to_datetime(data_6135['FROMTIME'])\n",
    "data_6132['FROMTIME'] = pd.to_datetime(data_6132['FROMTIME'])\n",
    "\n",
    "data_5690.index=data_5690['FROMTIME']\n",
    "data_5684.index=data_5684['FROMTIME']\n",
    "data_6135.index=data_6135['FROMTIME']\n",
    "data_6132.index=data_6132['FROMTIME']\n",
    "\n",
    "data_5690['FROMTIME-Time'] = data_5690['FROMTIME'].apply(findTime)  \n",
    "data_5690['FROMTIME-Date'] = data_5690['FROMTIME'].apply(findDate)  \n",
    "\n",
    "data_5684['FROMTIME-Time'] = data_5684['FROMTIME'].apply(findTime) \n",
    "data_5684['FROMTIME-Date'] = data_5684['FROMTIME'].apply(findDate) \n",
    "\n",
    "data_6135['FROMTIME-Time'] = data_6135['FROMTIME'].apply(findTime) \n",
    "data_6135['FROMTIME-Date'] = data_6135['FROMTIME'].apply(findDate)\n",
    "\n",
    "data_6132['FROMTIME-Time'] = data_6132['FROMTIME'].apply(findTime)  \n",
    "data_6132['FROMTIME-Date'] = data_6132['FROMTIME'].apply(findDate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM,Bidirectional\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras import regularizers\n",
    "from sklearn import metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "xianquan_5690 = data_5690.resample(datetime.timedelta(seconds=5 * 60)).sum()\n",
    "df_5690= xianquan_5690.copy()\n",
    "\n",
    "df_5690['FROMTIME'] = pd.to_datetime(df_5690.index)\n",
    "df_5690['FROMTIME-Time'] = df_5690[\"FROMTIME\"].apply(findTime)  # 找到时间\n",
    "df_5690['FROMTIME-Date'] = df_5690[\"FROMTIME\"].apply(findDate)  # 找到日期\n",
    "df_5690=df_5690[(pd.to_datetime(df_5690[\"FROMTIME-Time\"],format = '%H:%M:%S')>= pd.to_datetime('07:00:00',format = '%H:%M:%S'))&(pd.to_datetime(df_5690[\"FROMTIME-Time\"],format = '%H:%M:%S')<= pd.to_datetime('22:00:00',format = '%H:%M:%S'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xianquan_5684 = data_5684.resample(datetime.timedelta(seconds=5 * 60)).sum()\n",
    "df_5684= xianquan_5684.copy()\n",
    "\n",
    "df_5684['FROMTIME'] = pd.to_datetime(df_5684.index)\n",
    "df_5684['FROMTIME-Time'] = df_5684[\"FROMTIME\"].apply(findTime)  # 找到时间\n",
    "df_5684['FROMTIME-Date'] = df_5684[\"FROMTIME\"].apply(findDate)  # 找到日期\n",
    "df_5684=df_5684[(pd.to_datetime(df_5684[\"FROMTIME-Time\"],format = '%H:%M:%S')>= pd.to_datetime('07:00:00',format = '%H:%M:%S'))&(pd.to_datetime(df_5684[\"FROMTIME-Time\"],format = '%H:%M:%S')<= pd.to_datetime('22:00:00',format = '%H:%M:%S'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xianquan_6135 = data_6135.resample(datetime.timedelta(seconds=5 * 60)).sum()\n",
    "df_6135= xianquan_6135.copy()\n",
    "\n",
    "df_6135['FROMTIME'] = pd.to_datetime(df_6135.index)\n",
    "df_6135['FROMTIME-Time'] = df_6135[\"FROMTIME\"].apply(findTime)  # 找到时间\n",
    "df_6135['FROMTIME-Date'] = df_6135[\"FROMTIME\"].apply(findDate)  # 找到日期\n",
    "df_6135=df_6135[(pd.to_datetime(df_6135[\"FROMTIME-Time\"],format = '%H:%M:%S')>= pd.to_datetime('07:00:00',format = '%H:%M:%S'))&(pd.to_datetime(df_6135[\"FROMTIME-Time\"],format = '%H:%M:%S')<= pd.to_datetime('22:00:00',format = '%H:%M:%S'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xianquan_6132 = data_6132.resample(datetime.timedelta(seconds=5 * 60)).sum()\n",
    "df_6132= xianquan_6132.copy()\n",
    "\n",
    "df_6132['FROMTIME'] = pd.to_datetime(df_6132.index)\n",
    "df_6132['FROMTIME-Time'] = df_6132[\"FROMTIME\"].apply(findTime)  # 找到时间\n",
    "df_6132['FROMTIME-Date'] = df_6132[\"FROMTIME\"].apply(findDate)  # 找到日期\n",
    "df_6132=df_6132[(pd.to_datetime(df_6132[\"FROMTIME-Time\"],format = '%H:%M:%S')>= pd.to_datetime('07:00:00',format = '%H:%M:%S'))&(pd.to_datetime(df_6132[\"FROMTIME-Time\"],format = '%H:%M:%S')<= pd.to_datetime('22:00:00',format = '%H:%M:%S'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prove spatial correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "co_6132=df_6132[(pd.to_datetime(df_6132['FROMTIME-Date'],format = '%Y-%m-%d')>= pd.to_datetime('2019-07-08',format = '%Y-%m-%d'))&(pd.to_datetime(df_6132[\"FROMTIME-Date\"],format = '%Y-%m-%d')<= pd.to_datetime('2019-07-14',format = '%Y-%m-%d'))]\n",
    "co_5690=df_5690[(pd.to_datetime(df_5690['FROMTIME-Date'],format = '%Y-%m-%d')>= pd.to_datetime('2019-07-08',format = '%Y-%m-%d'))&(pd.to_datetime(df_5690[\"FROMTIME-Date\"],format = '%Y-%m-%d')<= pd.to_datetime('2019-07-14',format = '%Y-%m-%d'))]\n",
    "co_5684=df_5684[(pd.to_datetime(df_5684['FROMTIME-Date'],format = '%Y-%m-%d')>= pd.to_datetime('2019-07-08',format = '%Y-%m-%d'))&(pd.to_datetime(df_5684[\"FROMTIME-Date\"],format = '%Y-%m-%d')<= pd.to_datetime('2019-07-14',format = '%Y-%m-%d'))]\n",
    "co_6135=df_6135[(pd.to_datetime(df_6135['FROMTIME-Date'],format = '%Y-%m-%d')>= pd.to_datetime('2019-07-08',format = '%Y-%m-%d'))&(pd.to_datetime(df_6135[\"FROMTIME-Date\"],format = '%Y-%m-%d')<= pd.to_datetime('2019-07-14',format = '%Y-%m-%d'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_6132=np.array(co_6132[\"COUNT\"])\n",
    "data_5690=np.array(co_5690[\"COUNT\"])\n",
    "data_5684=np.array(co_5684[\"COUNT\"])\n",
    "data_6135=np.array(co_6135[\"COUNT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize= (20,5))\n",
    "ax = fig.add_subplot(111)\n",
    "d = '2019-07-08 08:00'\n",
    "x = pd.date_range(d, periods=168, freq='5min')\n",
    "\n",
    "ax.plot(data_6132,label='4694-4697')\n",
    "ax.plot(data_5690,label='4697-4806')\n",
    "ax.plot(data_5684,label='4706-4697')\n",
    "ax.plot(data_6135,label='4651-4697')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spearman correlation coefficient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "x1=pd.Series(data_5684)\n",
    "y1=pd.Series(data_6135)\n",
    "\n",
    "n=x1.count()\n",
    "\n",
    "x1.index=np.arange(n)\n",
    "y1.index=np.arange(n)\n",
    "  \n",
    "d=(x1.sort_values().index-y1.sort_values().index)**2\n",
    "dd=d.to_series().sum()\n",
    "\n",
    "p=1-n*dd/(n*(n**2-1))\n",
    "\n",
    "r=x1.corr(y1,method='spearman')\n",
    "print(r,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient between 6132 and 5690 is 0.438; The correlation coefficient between 6132 and 5684 is 0.537; The correlation coefficient between 6132 and 6135 is 0.782. The correlation coefficient between 5690 and 5684 was 0.685. The correlation coefficient between 5690 and 6135 was 0.512. The correlation coefficient between 5684 and 6135 is 0.472"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of correlation with 5690 coil is: 5684>6135>6132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Temporal Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing, establishing spatial-temporal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findTime(x):\n",
    "    return x.time()\n",
    "def findDate(x):\n",
    "    return x.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_5690=pd.read_csv('train_data/data/5690.csv')\n",
    "data_5684=pd.read_csv('train_data/data/5684.csv')\n",
    "data_6135=pd.read_csv('train_data/data/6135.csv')\n",
    "data_6132=pd.read_csv('train_data/data/6132.csv')\n",
    "allframes=pd.DataFrame()\n",
    "for road_id in [5684,5690,6135,6132]:\n",
    "    data_road_id=pd.read_csv('train_data/data/%s.csv'%road_id)\n",
    "    data_road_id['FROMTIME'] = pd.to_datetime(data_road_id['FROMTIME'])\n",
    "    data_road_id.index = data_road_id['FROMTIME']\n",
    "    road_data1= data_road_id.resample(datetime.timedelta(seconds=5 * 60)).sum()\n",
    "    allframes=pd.concat([allframes,road_data1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allframes.to_csv('train_data/cnn_lstm/allframes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group and standardize data by time series\n",
    "def group_by_time():\n",
    "    \"\"\"\n",
    "    :param key: group by time\n",
    "    :param zscore: z score standardization\n",
    "    :param grouped: The result of grouping and standardizing the data by time\n",
    "    :param vehicles: Convert the data into matrix form\n",
    "    \"\"\"\n",
    "    frame = frames\n",
    "    frame['FROMTIME'] = pd.to_datetime(frame['FROMTIME'], format='%Y-%m-%d %H:%M:%S')\n",
    "    values = frame.groupby('FROMTIME')['COUNT'].apply(list)\n",
    "    vehicles = []\n",
    "    \n",
    "    for i in range(len(values)):\n",
    "        count=0\n",
    "        for j in range(len(values[i])):\n",
    "            if values[i][j]<0.01:\n",
    "                count+=1\n",
    "        if count==4:\n",
    "            pass\n",
    "        else:\n",
    "            vehicles.append(values[i])\n",
    "    vehicles = np.asarray(vehicles)\n",
    "    #vechicles = vehicles.reshape((196, 288*FILE_NUMS))\n",
    "    #vechicles = np.array([np.reshape(x, (288, FILE_NUMS)) for x in vechicles])\n",
    "    return vehicles\n",
    "\n",
    "frames=pd.read_csv('train_data/cnn_lstm/allframes.csv')\n",
    "frames['FROMTIME'] = pd.to_datetime(frames['FROMTIME'])\n",
    "frames['FROMTIME-Time'] = frames['FROMTIME'].apply(findTime)  # 找到时间\n",
    "frames['FROMTIME-Date'] = frames['FROMTIME'].apply(findDate)  # 找到日期\n",
    "frames=frames[(pd.to_datetime(frames[\"FROMTIME-Time\"],format = '%H:%M:%S')>= pd.to_datetime('07:00:00',format = '%H:%M:%S'))&(pd.to_datetime(frames[\"FROMTIME-Time\"],format = '%H:%M:%S')<= pd.to_datetime('22:00:00',format = '%H:%M:%S'))]\n",
    "vehicles = group_by_time()\n",
    "vehicles = [i for i in vehicles if len(i)==4]\n",
    "\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "samples = scaler.fit_transform(vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames_tmp=frames[pd.to_datetime(frames[\"FROMTIME-Date\"],format = '%Y-%m-%d')==pd.to_datetime('2019-07-01',format = '%Y-%m-%d')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames_tmp.index=frames_tmp[\"FROMTIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(frames_tmp[\"COUNT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "split data\n",
    "\"\"\"\n",
    "\n",
    "class DataSet(object):\n",
    "    def __init__(self, flow, labels):\n",
    "        self._index_in_epoch = 0\n",
    "        self._epochs_completed = 0\n",
    "        self._flow = flow\n",
    "        self._labels = labels\n",
    "        self._num_examples = flow.shape[0]\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def flow(self):\n",
    "        return self._flow\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "\n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "\n",
    "    def next_batch(self, batch_size, shuffle = True):\n",
    "        \"\"\"\n",
    "        Return the next 'batch_size' examples from this dataset\n",
    "        \"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        # Shuffle for the first epoch\n",
    "        if start == 0 and self._epochs_completed == 0 and shuffle:\n",
    "            idx = np.arange(self._num_examples)\n",
    "            np.random.shuffle(idx)\n",
    "            self._flow = self.flow[idx]\n",
    "            self._labels = self.labels[idx]\n",
    "\n",
    "        # Go to the next epoch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            # Finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            flow_rest_part = self._flow[start:self._num_examples]\n",
    "            labels_rest_part = self._labels[start:self._num_examples]\n",
    "\n",
    "            # Shuffle the data\n",
    "            if shuffle:\n",
    "                idx0 = numpy.arrange(self._num_examples)\n",
    "                numpy.random.shuffle(idx0)\n",
    "                self._flow = self.flow[idx0]\n",
    "                self._labels = self.labels[idx0]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples\n",
    "            end = self._index_in_epoch\n",
    "            flow_new_part = self._flow[start:end]\n",
    "            labels_new_part = self._labels[start:end]\n",
    "            return numpy.concatenate((flow_rest_part, flow_new_part), axis=0), numpy.concatenate((labels_rest_part, labels_new_part), axis=0)\n",
    "\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._flow[start:end], self._labels[start:end]\n",
    "\n",
    "def create_data_sets():\n",
    "    sample = samples\n",
    "    look_back = 12\n",
    "    interval = 0\n",
    "    flow, labels = [], []\n",
    "    \n",
    "    for i in range(len(sample)-look_back-interval):\n",
    "        flow.append(sample[i:(i+look_back)])\n",
    "        labels.append(sample[i+look_back+interval])\n",
    "    return np.asarray(flow), np.asarray(labels)\n",
    "\n",
    "\n",
    "def read_data_sets():\n",
    "    flow, labels = create_data_sets()\n",
    "    validation_size = 4157\n",
    "    train_flow, test_flow, train_labels, test_labels = train_test_split(flow, \n",
    "                                                                        labels, \n",
    "                                                                        test_size = 0.2,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    validation_flow = train_flow[validation_size:]\n",
    "    validation_labels = train_labels[validation_size:]\n",
    "    train_flow = train_flow[:validation_size]\n",
    "    train_labels = train_labels[:validation_size]\n",
    "\n",
    "    train = DataSet(train_flow, train_labels)\n",
    "    test = DataSet(test_flow, test_labels)\n",
    "    validation = DataSet(validation_flow, validation_labels)\n",
    "    return base.Datasets(train=train, validation=validation, test=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flow,label=create_data_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, Dropout, Flatten, LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras.backend as K\n",
    "\n",
    "# import input_data\n",
    "\n",
    "# Convolution\n",
    "kernel_size = [2, 2, 4]\n",
    "\n",
    "# Training\n",
    "time_steps = 12\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "\n",
    "# load model\n",
    "print('loading data...')\n",
    "\n",
    "pems = read_data_sets()\n",
    "flow_train = pems.train.flow\n",
    "flow_train = flow_train.reshape((flow_train.shape[0], time_steps, 4, 1))\n",
    "labels_train = pems.train.labels\n",
    "flow_test = pems.test.flow\n",
    "flow_test = flow_test.reshape((flow_test.shape[0], time_steps, 4, 1))\n",
    "labels_test = pems.test.labels\n",
    "flow_validation = pems.validation.flow\n",
    "flow_validation = flow_validation.reshape((flow_validation.shape[0], time_steps, 4, 1))\n",
    "labels_validation = pems.validation.labels\n",
    "\n",
    "# construct model \n",
    "print('build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# the first convolution layer\n",
    "model.add(TimeDistributed(Conv1D(40,\n",
    "                 kernel_size[1],\n",
    "                 strides=1,\n",
    "                 padding='valid'), input_shape=[time_steps, 4, 1]))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "\n",
    "model.add(TimeDistributed(Conv1D(40, kernel_size[1], padding='valid')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "\n",
    "model.add(TimeDistributed(Conv1D(40, kernel_size[0], padding='valid')))\n",
    "model.add(TimeDistributed(BatchNormalization()))\n",
    "model.add(TimeDistributed(Activation('relu')))\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(Dense(4))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(128))\n",
    "model.add(Activation('tanh'))  \n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(4))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model.load_weights(\"Model/cnn_lstm_final.h5\")\n",
    "\n",
    "model.compile(loss='mean_squared_error', \n",
    "              optimizer='rmsprop',\n",
    "              metrics=['mae', 'cosine'])\n",
    "\n",
    "filepath = \"train_data/model/cnn_lstm.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# train\n",
    "print('Train...')\n",
    "\n",
    "model.fit(flow_train, labels_train, \n",
    "#          validation_split=0.33,\n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size,\n",
    "#           callbacks=callbacks_list,\n",
    "          validation_data=(flow_validation, labels_validation))\n",
    "\n",
    "score = model.evaluate(flow_test, labels_test, batch_size=batch_size)\n",
    "print('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('train_data/model/cnn_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### result comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.dates as md\n",
    "import matplotlib.ticker as mt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from keras.models import load_model\n",
    "import math\n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Percentage Error\n",
    "    Calculate the mape.\n",
    "\n",
    "    # Arguments\n",
    "        y_true: List/ndarray, ture data.\n",
    "        y_pred: List/ndarray, predicted data.\n",
    "    # Returns\n",
    "        mape: Double, result data for train.\n",
    "    \"\"\"\n",
    "\n",
    "    y = [x for x in y_true if x > 0]\n",
    "    y_pred = [y_pred[i] for i in range(len(y_true)) if y_true[i] > 0]\n",
    "\n",
    "    num = len(y_pred)\n",
    "    sums = 0\n",
    "\n",
    "    for i in range(num):\n",
    "        tmp = abs(y[i] - y_pred[i]) / y[i]\n",
    "        sums += tmp\n",
    "\n",
    "    mape = sums * (100 / num)\n",
    "\n",
    "    return mape\n",
    "def eva_regress(y_true, y_pred):\n",
    "    \"\"\"Evaluation\n",
    "    evaluate the predicted resul.\n",
    "\n",
    "    # Arguments\n",
    "        y_true: List/ndarray, ture data.\n",
    "        y_pred: List/ndarray, predicted data.\n",
    "    \"\"\"\n",
    "\n",
    "    mape = MAPE(y_true, y_pred)\n",
    "    vs = metrics.explained_variance_score(y_true, y_pred)\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    print('explained_variance_score:%f' % vs)\n",
    "    print('mape:%f%%' % mape)\n",
    "    print('mae:%f' % mae)\n",
    "    print('mse:%f' % mse)\n",
    "    print('rmse:%f' % math.sqrt(mse))\n",
    "    print('r2:%f' % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flow, labels = create_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_steps = 12\n",
    "\n",
    "# scaler = preprocessing.MinMaxScaler()\n",
    "# samples = scaler.fit_transform(vehicles)\n",
    "\n",
    "\n",
    "flow, labels = create_data_sets()\n",
    "x_test = flow_validation\n",
    "\n",
    "labels_test = labels_validation\n",
    "\n",
    "cnn_lstm_final_model = load_model('train_data/model/cnn_lstm.h5')\n",
    "\n",
    "y_test = scaler.inverse_transform(labels_test)\n",
    "\n",
    "\n",
    "x_test= x_test.reshape((x_test.shape[0], time_steps, 4, 1))\n",
    "pred = cnn_lstm_final_model.predict(x_test)\n",
    "cnn_lstm_final_pred = scaler.inverse_transform(pred)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.figure(figsize= (20,5))\n",
    "    plt.plot(y_test[40:,i])\n",
    "    plt.plot(cnn_lstm_final_pred[40:,i])\n",
    "for i in range(4):\n",
    "    eva_regress(y_test[40:,i], cnn_lstm_final_pred[40:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flow, labels = create_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flow, labels = create_data_sets()\n",
    "x_test = flow[5290:]\n",
    "labels_test = labels[5290:]\n",
    "\n",
    "cnn_lstm_final_model = load_model('train_data/model/cnn_lstm.h5')\n",
    "\n",
    "y_test = scaler.inverse_transform(labels_test)\n",
    "\n",
    "\n",
    "x_test= x_test.reshape((x_test.shape[0], time_steps, 4, 1))\n",
    "pred = cnn_lstm_final_model.predict(x_test)\n",
    "cnn_lstm_final_pred = scaler.inverse_transform(pred)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.figure(figsize= (20,5))\n",
    "    plt.plot(y_test[:,i])\n",
    "    plt.plot(cnn_lstm_final_pred[:,i])\n",
    "for i in range(4):\n",
    "    eva_regress(y_test[:,i], cnn_lstm_final_pred[:,i])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
